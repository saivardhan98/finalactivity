# -*- coding: utf-8 -*-
"""devops

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eSQImzcN7fYpbFkfmAkm8XQkErlPkK3W
"""

import numpy as np
import pandas as pd

data = pd.read_excel("Bank_Personal_Loan_Modelling.xlsx")

data.head()

data.describe()

data.shape

data.columns

data.dtypes

data.isnull().sum()

data.drop(['ID','ZIP Code'],axis = 1,inplace =True)

import seaborn as sns
import matplotlib.pyplot as plt

cat_attr = (['Education','Securities Account','Family','CD Account','Online','CreditCard'])
data[cat_attr] = data[cat_attr].astype('category')
num_attr = (['Age','Experience','Income','CCAvg','Mortgage'])

for col in num_attr:
  q1=data[col].quantile(0.25)
  q3=data[col].quantile(0.70)
  iqr = q3-q1
  filter1 = (data[col]>=q1 - 1.5*iqr) & (data[col]<=q3+1.5*iqr)
  data=data.loc[filter1]

data.isnull().sum()

data.dtypes

data.dtypes

for i in data:
    print(data[i].nunique())
    print(data[i].value_counts())

data.dtypes

data.head()

data.dtypes

y=data["Personal Loan"]
X=data.drop('Personal Loan',axis=1)
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.2,random_state=123)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score
from sklearn.metrics import confusion_matrix
LR=LogisticRegression(class_weight='balanced',solver='newton-cg')
LR.fit(X_train,y_train)
X_train_pred=LR.predict(X_train)
X_test_pred=LR.predict(X_test)
print('========Train=======')
print(f"Confusion Matrix \n{confusion_matrix(y_train, X_train_pred)}")
print('========Test=======')
print(f"Confusion Matrix \n{confusion_matrix(y_test, X_test_pred)}")
print("ACCURACY")
print(accuracy_score(y_train,X_train_pred))
print(accuracy_score(y_test,X_test_pred))
print("RECALL")
print(recall_score(y_train,X_train_pred))
print(recall_score(y_test,X_test_pred))

from sklearn import metrics
import matplotlib.pyplot as plt
auc = metrics.roc_auc_score(y_test, X_test_pred)
fpr, tpr, _ = metrics.roc_curve(y_test,  X_test_pred)
#create ROC curve
plt.plot(fpr,tpr,label="AUC="+str(auc))
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc=4)
plt.show()

from sklearn.tree import DecisionTreeClassifier
DT=DecisionTreeClassifier(class_weight='balanced',criterion = 'gini', max_leaf_nodes = 15, min_samples_split = 12)
DT.fit(X_train,y_train)
X_train_pred1=DT.predict(X_train)
X_test_pred1=DT.predict(X_test)
print('========Train=======')
print(f"Confusion Matrix \n{confusion_matrix(y_train, X_train_pred)}")
print('========Test=======')
print(f"Confusion Matrix \n{confusion_matrix(y_test, X_test_pred)}")
print("ACCURACY")
print(accuracy_score(y_train,X_train_pred1))
print(accuracy_score(y_test,X_test_pred1))
print("RECALL")
print(recall_score(y_train,X_train_pred1))
print(recall_score(y_test,X_test_pred1))

from sklearn import metrics
import matplotlib.pyplot as plt
auc = metrics.roc_auc_score(y_test, X_test_pred1)
fpr, tpr, _ = metrics.roc_curve(y_test,  X_test_pred1)
#create ROC curve
plt.plot(fpr,tpr,label="AUC="+str(auc))
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc=4)
plt.show()



from sklearn.ensemble import RandomForestClassifier
RF=RandomForestClassifier(class_weight='balanced',criterion = 'gini', max_leaf_nodes = 15, min_samples_split = 12)
RF.fit(X_train,y_train)
X_train_pred3=RF.predict(X_train)
X_test_pred3=RF.predict(X_test)
print('========Train=======')
print(f"Confusion Matrix \n{confusion_matrix(y_train, X_train_pred3)}")
print('========Test=======')
print(f"Confusion Matrix \n{confusion_matrix(y_test, X_test_pred3)}")
print("ACCURACY")
print(accuracy_score(y_train,X_train_pred3))
print(accuracy_score(y_test,X_test_pred3))
print("RECALL")
print(recall_score(y_train,X_train_pred3))
print(recall_score(y_test,X_test_pred3))

from sklearn.naive_bayes import GaussianNB
NBmodel = GaussianNB().fit(X_train,y_train) 
X_train_pred2 = NBmodel.predict(X_train)  
X_test_pred2 = NBmodel.predict(X_test) #predict on test data
print('========Train=======')
print(f"Confusion Matrix \n{confusion_matrix(y_train, X_train_pred)}")
print('========Test=======')
print(f"Confusion Matrix \n{confusion_matrix(y_test, X_test_pred)}")
print("ACCURACY")
print(accuracy_score(y_train,X_train_pred2))
print(accuracy_score(y_test,X_test_pred2))
print("RECALL")
print(recall_score(y_train,X_train_pred2))
print(recall_score(y_test,X_test_pred2))

from sklearn import metrics
import matplotlib.pyplot as plt
auc = metrics.roc_auc_score(y_test, X_test_pred2)
fpr, tpr, _ = metrics.roc_curve(y_test,  X_test_pred2)
#create ROC curve
plt.plot(fpr,tpr,label="AUC="+str(auc))
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc=4)
plt.show()

from sklearn.neighbors import KNeighborsClassifier
KNN = KNeighborsClassifier(n_neighbors=3,metric="euclidean")
KNN.fit(X_train,y_train)
X_train_pred4=KNN.predict(X_train)
X_test_pred4=KNN.predict(X_test)
print('========Train=======')
print(f"Confusion Matrix \n{confusion_matrix(y_train, X_train_pred4)}")
print('========Test=======')
print(f"Confusion Matrix \n{confusion_matrix(y_test, X_test_pred4)}")
print("ACCURACY")
print(accuracy_score(y_train,X_train_pred4))
print(accuracy_score(y_test,X_test_pred4))
print("RECALL")
print(recall_score(y_train,X_train_pred4))
print(recall_score(y_test,X_test_pred4))

from sklearn import metrics
import matplotlib.pyplot as plt
auc = metrics.roc_auc_score(y_test, X_test_pred4)
fpr, tpr, _ = metrics.roc_curve(y_test,  X_test_pred4)
#create ROC curve
plt.plot(fpr,tpr,label="AUC="+str(auc))
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc=4)
plt.show()

from sklearn.svm import SVC
Svm = SVC(C=0.1,kernel="linear",class_weight="balanced")
Svm.fit(X_train,y_train)
X_train_pred5=Svm.predict(X_train)
X_test_pred5=Svm.predict(X_test)
print('========Train=======')
print(f"Confusion Matrix \n{confusion_matrix(y_train, X_train_pred5)}")
print('========Test=======')
print(f"Confusion Matrix \n{confusion_matrix(y_test, X_test_pred5)}")
print("ACCURACY")
print(accuracy_score(y_train,X_train_pred))
print(accuracy_score(y_test,X_test_pred5))
print("RECALL")
print(recall_score(y_train,X_train_pred5))
print(recall_score(y_test,X_test_pred5))

from sklearn import metrics
import matplotlib.pyplot as plt
auc = metrics.roc_auc_score(y_test, X_test_pred5)
fpr, tpr, _ = metrics.roc_curve(y_test,  X_test_pred5)
#create ROC curve
plt.plot(fpr,tpr,label="AUC="+str(auc))
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc=4)
plt.show()

from sklearn.ensemble import GradientBoostingClassifier
GDC = GradientBoostingClassifier(n_estimators=50,
                         learning_rate=1)
GDC.fit(X_train,y_train)
X_train_pred6=GDC.predict(X_train)
X_test_pred6=GDC.predict(X_test)
print('========Train=======')
print(f"Confusion Matrix \n{confusion_matrix(y_train, X_train_pred6)}")
print('========Test=======')
print(f"Confusion Matrix \n{confusion_matrix(y_test, X_test_pred6)}")
print("ACCURACY")
print(accuracy_score(y_train,X_train_pred6))
print(accuracy_score(y_test,X_test_pred6))
print("RECALL")
print(recall_score(y_train,X_train_pred6))
print(recall_score(y_test,X_test_pred6))

from sklearn import metrics
import matplotlib.pyplot as plt
auc = metrics.roc_auc_score(y_test, X_test_pred6)
fpr, tpr, _ = metrics.roc_curve(y_test,  X_test_pred6)
#create ROC curve
plt.plot(fpr,tpr,label="AUC="+str(auc))
plt.ylabel('True Positive Rate')
plt.xlabel('False Positive Rate')
plt.legend(loc=4)
plt.show()
import pickle
pickle_out = open("model.pkl","wb")
pickle.dump(Svm, pickle_out)
pickle_out.close()
